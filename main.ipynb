{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, regularizers, callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a dict which consists label for each image path and its particular data.\n",
    "def load_file(file_path, label):\n",
    "\n",
    "    # declare the folder name\n",
    "    folder_name = file_path.split(\"/\")[-1]\n",
    "    # declare output list\n",
    "    out_list = []\n",
    "    # load every file that .png format\n",
    "    for image_path in glob.glob(file_path + \"/*.*\"):\n",
    "        # read image file\n",
    "        # image = imageio.imread(image_path)\n",
    "        image = imageio.v2.imread(image_path)   \n",
    "        # print(image_path)\n",
    "        # declare temporary dict dtype\n",
    "        temp = {}\n",
    "        # set the file name\n",
    "        temp[\"name\"] = image_path.split(\"/\")[-1]\n",
    "        # set the file label, 0 for non defect. 1 for defect\n",
    "        temp[\"label\"] = label\n",
    "\n",
    "        # There are somes images are tensor dtype\n",
    "        # Thus I fix by selecting only a tensor index zero\n",
    "        try:   \n",
    "            temp[\"data\"] = image[:,:,0].astype(\"int\") \n",
    "        except:\n",
    "            # normal case\n",
    "            temp[\"data\"] = image.astype(\"int\")\n",
    "        # append temp into output list\n",
    "        out_list.append(temp)\n",
    "    # print process status by checking size of output list\n",
    "    if len(out_list) == 0:\n",
    "        print(\"loading files from folder: {} is failed\".format(folder_name))\n",
    "    else:\n",
    "        print(\"loading file from folder: {} is successful\".format(folder_name))\n",
    "    # convert list into numpy array dtype\n",
    "    return np.array(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_images_path      =  r\"A:/TechieYan projects/AI/Completed/Identifying Defects in the Various Fabrics using Convolutional Neural Networks/FABRIC/Defect\"\n",
    "non_defect_images_path1 =  r\"A:/TechieYan projects/AI/Completed/Identifying Defects in the Various Fabrics using Convolutional Neural Networks/FABRIC/Normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file from folder: Defect is successful\n",
      "loading file from folder: Normal is successful\n"
     ]
    }
   ],
   "source": [
    "defect_images = load_file(file_path=defect_images_path, label=1)\n",
    "non_defect_images = load_file(file_path=non_defect_images_path1, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2251,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defect_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_defect_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare and clean the data to avoid error during model fitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Size: 2000\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(non_defect_images)\n",
    "np.random.shuffle(defect_images)\n",
    "\n",
    "# the class size is the min length compared with defect-free and defect images, we do this in orde to balance the dataset.\n",
    "if defect_images.shape[0] <= non_defect_images.shape[0]:\n",
    "  class_size = defect_images.shape[0]\n",
    "else:\n",
    "  class_size = non_defect_images.shape[0]\n",
    "print(\"Class Size:\", class_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Size: 2000\n",
      "(4000, 255, 255, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2000, 2000], dtype=int64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we shuffle the order of defect-free and defect images\n",
    "np.random.shuffle(non_defect_images)\n",
    "np.random.shuffle(defect_images)\n",
    "\n",
    "# the class size is the min length compared with defect-free and defect images, we do this in orde to balance the dataset.\n",
    "if defect_images.shape[0] <= non_defect_images.shape[0]:\n",
    "  class_size = defect_images.shape[0] \n",
    "else:\n",
    "  non_defect_images.shape[0]\n",
    "print(\"Class Size:\", class_size)\n",
    "\n",
    "# Concatenate both the datasets with size as class_size.\n",
    "dataset = np.concatenate((defect_images[:class_size], non_defect_images[:class_size]))\n",
    "\n",
    "# create an empty matrix X of 256x4096 and has dataset length row, which holds all the data i.e images from dataset.\n",
    "# Independent Features -> X\n",
    "X = np.empty([dataset.shape[0], 255, 255]).astype(int)\n",
    "\n",
    "# create vector y which has dataset length, which holds all the labels for our data, this is jsut similar to partitioning the data before splitting, \n",
    "# Target_variable -> y\n",
    "y = np.empty(dataset.shape[0]).astype(int)\n",
    "\n",
    "# assign the X,y one-by-one\n",
    "for i in range(dataset.shape[0]):\n",
    "    X[i] = dataset[i][\"data\"]\n",
    "    y[i] = dataset[i][\"label\"]\n",
    "\n",
    "# since Keras acquire the Image input in a tensor type -> we reshape X\n",
    "X = X.reshape(X.shape[0], 255, 255, 1)\n",
    "print(X.shape)\n",
    "\n",
    "# display size of the label 0 and label 1 \n",
    "np.unique(y, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 249, 249, 16)      800       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 249, 249, 16)     64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 249, 249, 16)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 124, 124, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 124, 124, 16)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 124, 124, 32)      12832     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 124, 124, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 124, 124, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 62, 62, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 62, 62, 64)        51264     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 62, 62, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 62, 62, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 31, 31, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 31, 31, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 61504)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                3936320   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,001,985\n",
      "Trainable params: 4,001,633\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1889a8f3760>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(image_shape=(256, 4096, 1), print_summary=True):\n",
    "    # initial model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # CONV layer: filter 16, stride 7x7\n",
    "    model.add(layers.Conv2D(16, (7, 7),input_shape=image_shape))\n",
    "    # Batch Normalization layer -> avoid overfitting\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # activation layer \n",
    "    model.add(layers.Activation('relu'))\n",
    "    # max pooling -> reduce image size\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    # droput later -> avoid overfitting\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "    # CONV layer: filter 32, stride 5x5\n",
    "    model.add(layers.Conv2D(32, (5, 5), padding=\"same\"))\n",
    "    # Batch Normalization layer -> avoid overfitting\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # activation layer \n",
    "    model.add(layers.Activation('relu'))\n",
    "    # max pooling -> reduce image size\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    # droput later -> avoid overfitting\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    # CONV layer: filter 64, stride 5x5\n",
    "    model.add(layers.Conv2D(64, (5, 5), padding=\"same\"))\n",
    "    # Batch Normalization layer -> avoid overfitting\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # activation layer \n",
    "    model.add(layers.Activation('relu'))\n",
    "    # max pooling -> reduce image size\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    # droput later -> avoid overfitting\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "    # flatten layer -> To convert from matrix to vector ie from multidimensional array to 1D array with single column.\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "\n",
    "    # Here we are creating an actual neural network which takes input as the flatten layer.\n",
    "    # fully connected layer -> nn layer with 64 nodes\n",
    "    model.add(layers.Dense(64))\n",
    "    # Batch Normalization layer -> avoid overfitting\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # activation layer \n",
    "    model.add(layers.Activation('relu'))\n",
    "    # droput later -> avoid overfitting\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "    # output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    # set model compiler\n",
    "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # show the CNN model detail\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "def train_model(model, xtrain, ytrain, xval, yval, n_epoch, batch_size):\n",
    "    # train CNN model\n",
    "    # batch size to reduce memory usage\n",
    "    # set early stopping to avoid overfitting\n",
    "    \n",
    "    earlystopping = EarlyStopping(monitor='val_accuracy', patience=2)\n",
    "    filepath = \"/content/drive/MyDrive/Fabric_Fault_detection/Fabric 2/model/weights-best-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    history = model.fit(xtrain, ytrain, epochs=n_epoch, batch_size=batch_size, validation_data=(xval, yval), callbacks=[callbacks_list])\n",
    "    return history\n",
    "\n",
    "create_model(image_shape=(255, 255, 1), print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Export CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: number of samples each class: (array([0, 1]), array([1600, 1600], dtype=int64))\n",
      "y_test: number of samples each class: (array([0, 1]), array([400, 400], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
    "print(\"y_train: number of samples each class: {}\".format(np.unique(y_train, return_counts=True)))\n",
    "print(\"y_test: number of samples each class: {}\".format(np.unique(y_test, return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 249, 249, 16)      800       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 249, 249, 16)     64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 249, 249, 16)      0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 124, 124, 16)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 124, 124, 16)      0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 124, 124, 32)      12832     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 124, 124, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 124, 124, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 62, 62, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 62, 62, 32)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 62, 62, 64)        51264     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 62, 62, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 62, 62, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 31, 31, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 31, 31, 64)        0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 61504)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                3936320   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,001,985\n",
      "Trainable params: 4,001,633\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = None\n",
    "cnn_model = create_model(image_shape=(255, 255, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "filepath = \"A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, earlystopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.5936 - accuracy: 0.6764\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.65938, saving model to A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\n",
      "288/288 [==============================] - 304s 1s/step - loss: 0.5936 - accuracy: 0.6764 - val_loss: 0.9034 - val_accuracy: 0.6594\n",
      "Epoch 2/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.7955\n",
      "Epoch 00002: val_accuracy improved from 0.65938 to 0.70938, saving model to A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\n",
      "288/288 [==============================] - 260s 901ms/step - loss: 0.4465 - accuracy: 0.7955 - val_loss: 1.1755 - val_accuracy: 0.7094\n",
      "Epoch 3/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.8375\n",
      "Epoch 00003: val_accuracy did not improve from 0.70938\n",
      "288/288 [==============================] - 257s 892ms/step - loss: 0.3718 - accuracy: 0.8375 - val_loss: 0.9447 - val_accuracy: 0.6906\n",
      "Epoch 4/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.3089 - accuracy: 0.8701\n",
      "Epoch 00004: val_accuracy improved from 0.70938 to 0.86875, saving model to A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\n",
      "288/288 [==============================] - 255s 887ms/step - loss: 0.3089 - accuracy: 0.8701 - val_loss: 0.3386 - val_accuracy: 0.8687\n",
      "Epoch 5/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.2222 - accuracy: 0.9160\n",
      "Epoch 00005: val_accuracy did not improve from 0.86875\n",
      "288/288 [==============================] - 274s 953ms/step - loss: 0.2222 - accuracy: 0.9160 - val_loss: 0.3688 - val_accuracy: 0.8438\n",
      "Epoch 6/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.9326\n",
      "Epoch 00006: val_accuracy improved from 0.86875 to 0.94063, saving model to A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\n",
      "288/288 [==============================] - 296s 1s/step - loss: 0.1842 - accuracy: 0.9326 - val_loss: 0.1181 - val_accuracy: 0.9406\n",
      "Epoch 7/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9417\n",
      "Epoch 00007: val_accuracy did not improve from 0.94063\n",
      "288/288 [==============================] - 320s 1s/step - loss: 0.1632 - accuracy: 0.9417 - val_loss: 0.2706 - val_accuracy: 0.8938\n",
      "Epoch 8/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1378 - accuracy: 0.9500\n",
      "Epoch 00008: val_accuracy did not improve from 0.94063\n",
      "288/288 [==============================] - 284s 985ms/step - loss: 0.1378 - accuracy: 0.9500 - val_loss: 0.1426 - val_accuracy: 0.9312\n",
      "Epoch 9/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9399\n",
      "Epoch 00009: val_accuracy did not improve from 0.94063\n",
      "288/288 [==============================] - 282s 979ms/step - loss: 0.1551 - accuracy: 0.9399 - val_loss: 0.3626 - val_accuracy: 0.8531\n",
      "Epoch 10/10\n",
      "288/288 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9556\n",
      "Epoch 00010: val_accuracy improved from 0.94063 to 0.95938, saving model to A:\\TechieYan projects\\AI\\Fabric_Fault_detection\\Fabric 2\\model\\weights_best.hdf5\n",
      "288/288 [==============================] - 283s 982ms/step - loss: 0.1185 - accuracy: 0.9556 - val_loss: 0.1156 - val_accuracy: 0.9594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28d48a63310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to run this, if you are loading the model, if not you cna run this to train your model again and save another weights file.\n",
    "# cnn_model.fit(X_train, y_train, batch_size=10, epochs=10, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = tf.keras.models.load_model(\"A:/TechieYan projects/AI/Completed/Identifying Defects in the Various Fabrics using Convolutional Neural Networks/FABRIC/model/weights_best.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09910967946052551, 0.9649999737739563)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, acc = my_model.evaluate(X_test, y_test, verbose=0)\n",
    "score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 17s 548ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = y_pred.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = len(y_pred)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(count):\n",
    "    if y_pred[i] > 0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "cf_matrix = confusion_matrix(y_pred,y_test)\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 255, 255, 1)\n"
     ]
    }
   ],
   "source": [
    "xi = np.array(X_test[88]).reshape(-1, 255, 255, 1)\n",
    "print(xi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "The Fabric is good.\n"
     ]
    }
   ],
   "source": [
    "y_pred = cnn_model.predict(xi)\n",
    "y_pred\n",
    "if y_pred > 0.5:\n",
    "    y_pred = 1\n",
    "    print(\"The Fabric is Defect.\")\n",
    "else:\n",
    "    y_pred = 0\n",
    "    print(\"The Fabric is good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "80bc14a6df0c4942103d0618f3b06907407c921bdce606ff519c2e4f7269e47f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
